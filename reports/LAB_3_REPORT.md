# Topic: Lexer & Scanner

### Course: Formal Languages & Finite Automata
### Author: Botnari Ciprian

## Theory
**Lexer** also known as a lexical analyzer, is a software component that reads source code and breaks it down into individual tokens based on the syntax rules of a programming language. It is typically the first stage of a compiler or interpreter and is responsible for performing lexical analysis of the source code.

## Objectives:

1. Understand what lexical analysis is.
2. Get familiar with the inner workings of a lexer/scanner/tokenizer.
3. Implement a sample lexer and show how it works.

## Implementation 

### TokenType
* Defines an enumeration called **TokenType** representing different types of tokens in Java. 
* It includes commands like **PRINT** and **PRINTLN**, keywords like **IF** and **ELSE**, variable types such as **BOOLEAN** and **VOID**, and an **END_OF_INPUT** token indicating the program's end.
```
public enum TokenType {
  // Commands
  PRINT, PRINTLN,

  // Keywords
  IF, ELSE, WHILE, FOR, RETURN,

  // Types
  BOOLEAN, INT, CHAR, STRING, FLOAT, DOUBLE, VOID,
  
  ...........
  
  // End of input
  END_OF_INPUT
}  
```

### Token
* Defines a class called **Token** with two private instance variables *type* and *lexeme*. 
* It also includes a constructor to create **Token** objects and a **toString()** method that returns a formatted string representation of a **Token** object.

```
public class Token {
  private final TokenType type;
  private final String lexeme;

  public Token(TokenType type, String lexeme) {
    this.type = type;
    this.lexeme = lexeme;
  }
  
  ...........
  
  @Override
  public String toString() {
    return String.format("(%s, %s)", type, lexeme);
  }
```

### Lexer
* Can parse a given input string into a list of **Token** objects, using a defined set of regular expressions/patterns that map to different types of Token (e.g. operators, keywords, identifiers, whitespace, etc.). 
* The **tokenize** method scans the input string and uses regular expressions to identify the longest matching token at the current position, adding the corresponding **Token** object to a list of tokens until the entire input has been consumed. 
* The **printTokens** method simply prints out the list of tokens generated by tokenize.
* The **getTokenPattern** method is a helper method that returns a regular expression pattern string for a given **TokenType**.
* Finally, the **TokenType** enum defines all the possible types of tokens that can be generated.
```
public class Lexer {
  private final String input;

  public Lexer(String input) {
    this.input = input;
  }

  public List<Token> tokenize() {
    // Iterate through the input string and attempts 
    // to match it with the regular expressions for each TokenType.

    // If a match is found, it creates a Token object with the 
    // matched text and TokenType and adds it to the list of Tokens.

    // If no match is found, the method throws an IllegalArgumentException.

    // Finally, the method adds an "END_OF_INPUT" token 
    // to the end of the token list and returns it.
  }

  private String getTokenPattern(TokenType type) {
    return switch (type) {
      case PRINT -> "print";
      case PRINTLN -> "println";
      case IF -> "if";
      ...........
      default -> null;
    };
  }

  public void printTokens() {
    List<Token> tokens = this.tokenize();
    for (Token token : tokens) {
      System.out.print(token + ", ");
    }
  }
}

```


## Results
===== 1. For input: 5+(6-2)*3/4 the tokens are: =====
* (INT, 5), (PLUS, +), (LEFT_PAREN, (), 
* (INT, 6), (MINUS, -), (INT, 2),
* (RIGHT_PAREN, )), (MULTIPLY, *), (INT, 3), 
* (DIVIDE, /), (INT, 4), (END_OF_INPUT, )

===== 2. For input: if (a == b) { return true; } the tokens are: =====
* (IF, if), (WHITESPACE,  ), (LEFT_PAREN, (), 
* (IDENTIFIER, a), (WHITESPACE,  ), (EQUALS, ==),
* (WHITESPACE,  ), (IDENTIFIER, b), (RIGHT_PAREN, )), 
* (WHITESPACE,  ), (LEFT_BRACE, {), (WHITESPACE,  ), 
* (RETURN, return), (WHITESPACE,  ), (BOOLEAN, true),
* (SEMICOLON, ;), (WHITESPACE,  ), (RIGHT_BRACE, }), (END_OF_INPUT, )

===== 3. For input: for (int i = 0; i < 10; i++) { print(i); } the tokens are: =====
* (FOR, for), (WHITESPACE,  ), (LEFT_PAREN, (), 
* (IDENTIFIER, int), (WHITESPACE,  ), (IDENTIFIER, i), 
* (WHITESPACE,  ), (ASSIGNMENT, =), (WHITESPACE,  ),
* (INT, 0), (SEMICOLON, ;), (WHITESPACE,  ), 
* (IDENTIFIER, i), (WHITESPACE,  ), (LESS_THAN, <),
* (WHITESPACE,  ), (INT, 10), (SEMICOLON, ;), 
* (WHITESPACE,  ), (IDENTIFIER, i), (PLUS, +),
* (PLUS, +), (RIGHT_PAREN, )), (WHITESPACE,  ), 
* (LEFT_BRACE, {), (WHITESPACE,  ), (PRINT, print), 
* (LEFT_PAREN, (), (IDENTIFIER, i), (RIGHT_PAREN, )), 
* (SEMICOLON, ;), (WHITESPACE,  ), (RIGHT_BRACE, }), (END_OF_INPUT, )

===== 4. For input: while (i < 10) { println(i); i++; } the tokens are: =====
* (WHILE, while), (WHITESPACE,  ), (LEFT_PAREN, (),
* (IDENTIFIER, i), (WHITESPACE,  ), (LESS_THAN, <),
* (WHITESPACE,  ), (INT, 10), (RIGHT_PAREN, )), 
* (WHITESPACE,  ), (LEFT_BRACE, {), (WHITESPACE,  ), 
* (PRINTLN, println), (LEFT_PAREN, (), (IDENTIFIER, i),
* (RIGHT_PAREN, )), (SEMICOLON, ;), (WHITESPACE,  ),
* (IDENTIFIER, i), (PLUS, +), (PLUS, +), 
* (SEMICOLON, ;), (WHITESPACE,  ), (RIGHT_BRACE, }), (END_OF_INPUT, )

===== 5. For input: char c = 'a'; the tokens are: =====
* (IDENTIFIER, char), (WHITESPACE,  ), (IDENTIFIER, c), 
* (WHITESPACE,  ), (ASSIGNMENT, =), (WHITESPACE,  ),
* (CHAR, 'a'), (SEMICOLON, ;), (END_OF_INPUT, )

## Conclusions 
In conclusion, the laboratory work on lexer has been a great learning experience for me. Through this work, I have achieved the following objectives:

Firstly, I have gained a thorough understanding of what lexical analysis is and its significance in programming. I have learned that lexical analysis is the process of breaking down a program into a sequence of tokens that can be easily processed by a parser. This helps to simplify the overall parsing process and make it more efficient.

Secondly, I have become familiar with the inner workings of a lexer/scanner/tokenizer. I have learned that a lexer is responsible for identifying and classifying the various tokens present in a program based on a set of rules defined by the programming language. I have also learned about the different techniques used by a lexer to recognize and tokenize various types of tokens.

Finally, I have implemented a sample lexer and demonstrated how it works. Through this exercise, I have gained practical experience in developing a lexer for a given programming language. I have learned about the various challenges involved in designing a lexer and how to overcome them.

Overall, this laboratory work has been an excellent opportunity for me to gain a deeper understanding of lexical analysis and its significance in programming. It has equipped me with the necessary skills and knowledge to develop lexers for various programming languages, which will be invaluable in my future endeavors as a programmer.


















